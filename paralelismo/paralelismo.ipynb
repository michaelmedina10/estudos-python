{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTIDADE_ARQUIVOS = 10\n",
    "\n",
    "DF = pd.DataFrame({\n",
    "    \"Produto\": [\"TV\", \"Playstation\", \"Celular\", \"Tablet\"],\n",
    "    \"Preco\": [2599.99, 4999.00, 1399.99, 1450.00]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_arquivos(quantidade: int = QUANTIDADE_ARQUIVOS) -> None:\n",
    "    for i in range(quantidade):\n",
    "        DF.to_csv(f\"arquivo_{i}.csv\")\n",
    "\n",
    "def excluir_arquivos():\n",
    "    subprocess.run(\"rm -f *.csv\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando arquivos\n",
    "gerar_arquivos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "Valor total é:  10448.98\n",
      "FIM EXEC: 0.019392013549804688\n"
     ]
    }
   ],
   "source": [
    "# Código original\n",
    "inicio = time.time()\n",
    "\n",
    "arquivos = os.listdir()\n",
    "for arquivo in arquivos:\n",
    "    if arquivo.endswith(\".csv\"):\n",
    "        tabela = pd.read_csv(arquivo)\n",
    "        total = tabela[\"Preco\"].sum()\n",
    "        print(\"Valor total é: \", total)\n",
    "\n",
    "print(f\"FIM EXEC: {time.time() - inicio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Valor total é: 10448.98', None, 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98']\n",
      "FIM EXEC: 3.02856707572937\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "arquivos = os.listdir()\n",
    "\n",
    "def calcular_preco_total(arquivo: str):\n",
    "    if arquivo.endswith(\".csv\"):\n",
    "        tabela = pd.read_csv(arquivo)\n",
    "        total = tabela[\"Preco\"].sum()\n",
    "        # Print nao funciona em parelelo\n",
    "        # print(\"Valor total é: \", total)\n",
    "        return f\"Valor total é: {total}\"\n",
    "\n",
    "resultado = Parallel(n_jobs=2)(delayed(calcular_preco_total)(arquivo) for arquivo in arquivos)\n",
    "print(resultado)\n",
    "print(f\"FIM EXEC: {time.time() - inicio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paralelismo\n",
    "\n",
    "Para funções rápidas, de alguns segundos não compensa usar multiprocessamento, mas para jobs que levam meia hora, talvez valha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threading (para tarefas I/O-bound):\n",
    "Para tarefas que dependem fortemente de operações de entrada e saída (I/O-bound), como leitura de arquivos ou chamadas de rede, o uso de threads pode ser mais eficaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Valor total é: 10448.98', None, 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98', 'Valor total é: 10448.98']\n",
      "FIM EXEC: 0.028217077255249023\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "def calcular_preco_total(arquivo: str):\n",
    "    if arquivo.endswith(\".csv\"):\n",
    "        tabela = pd.read_csv(arquivo)\n",
    "        total = tabela[\"Preco\"].sum()\n",
    "        return f\"Valor total é: {total}\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    resultados = list(executor.map(calcular_preco_total, arquivos))\n",
    "\n",
    "print(resultados)\n",
    "print(f\"FIM EXEC: {time.time() - inicio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing (para tarefas CPU-bound):\n",
    "Para tarefas que consomem muito CPU (CPU-bound), como cálculos intensivos, a biblioteca multiprocessing pode ser uma escolha melhor, já que ela cria processos separados que podem usar múltiplos núcleos.\n",
    "\n",
    "O código abaixo iria funcionar melhor em  um script .py, no jupyter ele não esta conseguindo serializar a função calcular_preco_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# inicio = time.time()\n",
    "\n",
    "# def calcular_preco_total(arquivo: str):\n",
    "#     if arquivo.endswith(\".csv\"):\n",
    "#         tabela = pd.read_csv(arquivo)\n",
    "#         total = tabela[\"Preco\"].sum()\n",
    "#         return f\"Valor total é: {total}\"\n",
    "\n",
    "# with Pool(processes=2) as pool:\n",
    "#     resultados = pool.map(calcular_preco_total, arquivos)\n",
    "\n",
    "# print(resultados)\n",
    "# print(f\"FIM EXEC: {time.time() - inicio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Excluir Arquivos\n",
    "excluir_arquivos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threading (ThreadPoolExecutor):\n",
    "Como funciona: Cria múltiplas threads dentro do mesmo processo. Threads compartilham o mesmo espaço de memória e são mais leves que processos, mas como o Python tem o GIL (Global Interpreter Lock), elas não conseguem executar código Python simultaneamente em múltiplos núcleos. No entanto, Threading é muito eficiente para tarefas I/O-bound (como leitura de arquivos ou chamadas de rede), pois essas tarefas não são bloqueadas pelo GIL.\n",
    "Quando usar: Ideal para operações I/O-bound onde o tempo de espera é maior que o tempo de CPU. Não é tão eficaz para tarefas CPU-bound.\n",
    "\n",
    "### Multiprocessing (multiprocessing.Pool):\n",
    "Como funciona: Cria múltiplos processos separados. Cada processo tem seu próprio espaço de memória, o que permite a execução simultânea em múltiplos núcleos de CPU. Como cada processo é isolado, isso evita o GIL e é ideal para tarefas CPU-bound.\n",
    "Quando usar: Quando você precisa utilizar múltiplos núcleos do CPU para tarefas pesadas (CPU-bound). Também pode ser usado para I/O-bound, mas com overhead maior devido à criação de processos.\n",
    "\n",
    "### Joblib (Parallel):\n",
    "Como funciona: Joblib pode usar tanto threads quanto processos. Por padrão, para operações em Python (não nativo), ele usa o backend loky, que cria processos separados. Ele é projetado para tarefas simples e eficazes, com uma interface fácil de usar.\n",
    "Quando usar: Para paralelizar loops e operações que se beneficiam da execução em múltiplos processos.\n",
    "\n",
    "### Pathos (ProcessingPool):\n",
    "Como funciona: Similar ao multiprocessing, Pathos usa processos separados, mas é mais robusto em termos de serialização, usando dill ao invés de pickle. Isso facilita a paralelização em ambientes como Jupyter Notebooks.\n",
    "Quando usar: Quando você precisa de multiprocessing em ambientes onde o multiprocessing padrão do Python tem problemas, como em Jupyter.\n",
    "\n",
    "### Dask:\n",
    "Como funciona: Dask é uma biblioteca mais complexa que pode criar tanto threads quanto processos. Ela também pode ser distribuída, permitindo que você rode tarefas em múltiplos nós de um cluster. Dask usa \"gráficos de tarefas\" para dividir grandes tarefas em várias menores, que podem ser executadas em paralelo.\n",
    "Quando usar: Ideal para grandes volumes de dados, tarefas distribuídas, ou quando você precisa de escalabilidade. É uma boa escolha para tarefas que precisam ser executadas tanto em um único computador quanto em clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
